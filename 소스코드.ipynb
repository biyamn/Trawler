{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twiiter_search_V1_7_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wC9XNHlPU-Oc",
        "outputId": "42e8c23b-ae70-48b3-e523-453126447341"
      },
      "source": [
        "# ----------------------패키지 임포트, API 인증----------------------\n",
        "!pip install pymysql\n",
        "!pip install selenium\n",
        "!pip install konlpy\n",
        "!pip install google-cloud-vision\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler, API\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import csv\n",
        "import pandas\n",
        "import re\n",
        "import konlpy\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from time import sleep\n",
        "import time\n",
        "import webbrowser\n",
        "import os \n",
        "import xlrd\n",
        "import pymysql\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "\n",
        "# API key 입력\n",
        "CONSUMER_KEY = ''     \n",
        "CONSUMER_SECRET = '' \n",
        "ACCESS_TOKEN = '' \n",
        "ACCESS_TOKEN_SECRET = ''    \n",
        "\n",
        "# API 접근\n",
        "auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
        "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
        "api = API(auth, wait_on_rate_limit = True) # 크롤링 횟수 제한 풀기\n",
        "\n",
        "\n",
        "#--------------------------------05/23 추가-----------------------------------\n",
        "#DB 연동\n",
        "data_words_db = pymysql.connect(\n",
        "    user='',\n",
        "    passwd='',\n",
        "    host='database-1.csxshuumxmig.ap-northeast-2.rds.amazonaws.com',\n",
        "    db='project',\n",
        "    charset='utf8mb4',\n",
        "    use_unicode=True\n",
        ")\n",
        "print('접속되었습니다')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymysql\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/52/a115fe175028b058df353c5a3d5290b71514a83f67078a6482cff24d6137/PyMySQL-1.0.2-py3-none-any.whl (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 23.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 30kB 17.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 15.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: pymysql\n",
            "Successfully installed pymysql-1.0.2\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 79.2MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/88/f817ef1af6f794e8f11313dcd1549de833f4599abcec82746ab5ed086686/JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 47.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, beautifulsoup4, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "Collecting google-cloud-vision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/51/e6321162877a2903ba3158737b944cf582a62b7f045e22864ab56b764adc/google_cloud_vision-2.3.1-py2.py3-none-any.whl (461kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-vision) (1.26.3)\n",
            "Collecting proto-plus>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/8a/61c5a9b9b6288f9b060b6e3d88374fc083953a29aeac7206616c2d3c9c8e/proto_plus-1.18.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (57.0.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (3.12.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.53.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (20.9)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.30.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.15.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2018.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.23.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0; extra == \"grpc\" in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.34.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.4.7)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (4.2.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (3.0.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (0.4.8)\n",
            "Installing collected packages: proto-plus, google-cloud-vision\n",
            "Successfully installed google-cloud-vision-2.3.1 proto-plus-1.18.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "접속되었습니다\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj4FHn2ZZ5L1",
        "outputId": "61dcf026-a7fb-4e1d-c6f7-43ae6327a027"
      },
      "source": [
        "#DB 접속\n",
        "cursor = data_words_db.cursor()\n",
        "sql = \"SELECT * FROM data_set;\"\n",
        "cursor.execute(sql)\n",
        "result = cursor.fetchall()\n",
        "df1 = pd.DataFrame(result)\n",
        "df1.columns = ['number','main_set','sub_set']\n",
        "#df1 = 기존의 단어셋\n",
        "\n",
        "#--------------------------------05/23 추가-----------------------------------\n",
        "df1_1=pd.DataFrame()\n",
        "# 빈 데이터 프레임 생성\n",
        "df1_sum = pd.concat([df1['main_set'],df1['sub_set']])\n",
        "#df1_sum에 기존의 단어셋인 main과 sub를 결합\n",
        "df1_sum=df1_sum.reset_index(drop=True) \n",
        "#인덱스 초기화를 통해 컬럼의 row 번호를 재정렬하고 drop=true로 컬럼명 제거\n",
        "print(df1_sum)\n",
        "print('\\n')\n",
        "\n",
        "df_final=pd.concat([df1_1,df1_sum],ignore_index=True)\n",
        "#df_final에 빈프레임(df1_1)과 기존 단어셋을 합친다(df1_sum)\n",
        "df_final.columns=['node_name']\n",
        "#df_final의 컬럼명을 node_name으로 정립\n",
        "print(df_final)\n",
        "\n",
        "# df1은 main_set과 sub_set이 있는 키워드 db\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0                 대딸방\n",
            "1                 룸싸롱\n",
            "2               섹스파트너\n",
            "3                  콜걸\n",
            "4                 마사지\n",
            "5                  안마\n",
            "6                  출장\n",
            "7                  후기\n",
            "8                  셔츠\n",
            "9                 노래방\n",
            "10                파트너\n",
            "11                 추천\n",
            "12    Samanth72483370\n",
            "13    SusanPr33929337\n",
            "14    oartearz4yxWzDy\n",
            "15          MouyKelly\n",
            "16      babyfat_cheek\n",
            "17      Lesli86294362\n",
            "18    Dvy9YigoGtyi0SN\n",
            "19    W3EPNyF7FpUX3fR\n",
            "20    EsnnqZaLZqkgas2\n",
            "21    KrWMc5llH3ifTK6\n",
            "22    y7o2AqV9hUKJBOv\n",
            "23                 유흥\n",
            "24                 섹시\n",
            "25                 69\n",
            "26                 조건\n",
            "27                 섹스\n",
            "28                 대행\n",
            "29                 오피\n",
            "30                 강남\n",
            "31                 만남\n",
            "32                 애인\n",
            "33                사이트\n",
            "34                 강추\n",
            "35         hyewon1000\n",
            "36    UN6R7Z97NvGBM3v\n",
            "37    AkpZxHUftv7fXG7\n",
            "38    Corneli15440797\n",
            "39        jujujip0702\n",
            "40    0KAIyLxN1zy3zzS\n",
            "41           hyoubeen\n",
            "42    YvetteS11564413\n",
            "43    kv8xqOAviEWHa0F\n",
            "44          vvoottee4\n",
            "45    6ea6CXAsx6iZ9Vz\n",
            "dtype: object\n",
            "\n",
            "\n",
            "          node_name\n",
            "0               대딸방\n",
            "1               룸싸롱\n",
            "2             섹스파트너\n",
            "3                콜걸\n",
            "4               마사지\n",
            "5                안마\n",
            "6                출장\n",
            "7                후기\n",
            "8                셔츠\n",
            "9               노래방\n",
            "10              파트너\n",
            "11               추천\n",
            "12  Samanth72483370\n",
            "13  SusanPr33929337\n",
            "14  oartearz4yxWzDy\n",
            "15        MouyKelly\n",
            "16    babyfat_cheek\n",
            "17    Lesli86294362\n",
            "18  Dvy9YigoGtyi0SN\n",
            "19  W3EPNyF7FpUX3fR\n",
            "20  EsnnqZaLZqkgas2\n",
            "21  KrWMc5llH3ifTK6\n",
            "22  y7o2AqV9hUKJBOv\n",
            "23               유흥\n",
            "24               섹시\n",
            "25               69\n",
            "26               조건\n",
            "27               섹스\n",
            "28               대행\n",
            "29               오피\n",
            "30               강남\n",
            "31               만남\n",
            "32               애인\n",
            "33              사이트\n",
            "34               강추\n",
            "35       hyewon1000\n",
            "36  UN6R7Z97NvGBM3v\n",
            "37  AkpZxHUftv7fXG7\n",
            "38  Corneli15440797\n",
            "39      jujujip0702\n",
            "40  0KAIyLxN1zy3zzS\n",
            "41         hyoubeen\n",
            "42  YvetteS11564413\n",
            "43  kv8xqOAviEWHa0F\n",
            "44        vvoottee4\n",
            "45  6ea6CXAsx6iZ9Vz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPLWKrq8JUgB",
        "outputId": "d003be13-e1af-473d-b6e5-d6c174d495ba"
      },
      "source": [
        "#DB 연동\n",
        "cursor = data_words_db.cursor()\n",
        "sql = \"SELECT * FROM data_strange;\"\n",
        "cursor.execute(sql)\n",
        "result = cursor.fetchall()\n",
        "strange = pd.DataFrame(result,columns=['number','words'])\n",
        "print(strange)\n",
        "print(len(strange))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    number words\n",
            "0        1  ㅁr사지\n",
            "1        2  ㅁし사지\n",
            "2        3  마人ㅏ지\n",
            "3        4  ㅁⓡ사지\n",
            "4        5   섹ㅅ|\n",
            "5        6   섹ㅅl\n",
            "6        7   섹ㅅi\n",
            "7        8   섹ㅅ卜\n",
            "8        9   섹ㅍr\n",
            "9       10   섹ㅍし\n",
            "10      11   섹ㅍⓡ\n",
            "11      12   섹ㅍ卜\n",
            "12      13   ㄷり행\n",
            "13      14   ㄷH행\n",
            "14      15   ㄷⓗ행\n",
            "15      16   ㉢ㅐ행\n",
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxWQE5ogJUgB",
        "outputId": "2bcb83ea-3972-4340-a92c-105d18e6c2e9"
      },
      "source": [
        "\n",
        "df_F = pd.DataFrame({'a':[],'b':[]})\n",
        "\n",
        "lst=[]\n",
        "for i in strange['words']:\n",
        "  lst.append(i)\n",
        "\n",
        "lsts=[]\n",
        "for j in df_final['node_name']:\n",
        "  lsts.append(j)\n",
        "\n",
        "df_F['a']=lst\n",
        "df_F['b']=lsts[:len(strange)]\n",
        "print(df_F)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       a                b\n",
            "0   ㅁr사지              대딸방\n",
            "1   ㅁし사지              룸싸롱\n",
            "2   마人ㅏ지            섹스파트너\n",
            "3   ㅁⓡ사지               콜걸\n",
            "4    섹ㅅ|              마사지\n",
            "5    섹ㅅl               안마\n",
            "6    섹ㅅi               출장\n",
            "7    섹ㅅ卜               후기\n",
            "8    섹ㅍr               셔츠\n",
            "9    섹ㅍし              노래방\n",
            "10   섹ㅍⓡ              파트너\n",
            "11   섹ㅍ卜               추천\n",
            "12   ㄷり행  Samanth72483370\n",
            "13   ㄷH행  SusanPr33929337\n",
            "14   ㄷⓗ행  oartearz4yxWzDy\n",
            "15   ㉢ㅐ행        MouyKelly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6lJFXBiaC3v",
        "outputId": "c74a88c9-e664-41df-fbea-4613d3561907"
      },
      "source": [
        "#--------------------------------05/23 추가-----------------------------------\n",
        "addr = ('/Users/jeon-ilsin/Desktop/main_beta-main')\n",
        "#df = pandas.read_csv('main_bad_words.csv', sep=',') \n",
        "# 추후 DB로 이동할 부분임\n",
        "#--------------------------------05/23 추가-----------------------------------\n",
        "\n",
        "\n",
        "def check_nan(value) :\n",
        "    try :\n",
        "        float(value)\n",
        "        return True\n",
        "    except ValueError :\n",
        "        return False\n",
        "\n",
        "\n",
        "num_needed = 20\n",
        "tweet_list = [] # 트위터 내용\n",
        "name_list = [] #닉네임\n",
        "screen_name_list = [] #사용자 아이디\n",
        "date_list = [] #날짜 저장\n",
        "time_list = [] #트위터 시간 저장\n",
        "url_list = [] #주소 저장\n",
        "tw_id = [] #아이디 중복 확인을 위한 저장 공간\n",
        "last_id = -1 # ID of last tweet seen\n",
        "nouns_list = [] # 한글만 뽑은 문장에서 실제 단어인지 검사\n",
        "ko_text = [] # 파악한 단어 한글만 뽑기\n",
        "snstype = [] #snstype넣기\n",
        "modify_time = [] # 검색시간\n",
        "time_now = datetime.datetime.now() #검색시간 패키지 진값\n",
        "media_list = [] #사진 링크\n",
        "id_str_list = [] #id값 저장칸\n",
        "\n",
        "tweet_list2 = [] # 트위터 내용\n",
        "name_list2 = [] #닉네임\n",
        "screen_name_list2 = [] #사용자 아이디\n",
        "date_list2 = [] #날짜 저장\n",
        "time_list2 = [] #트위터 시간 저장\n",
        "url_list2 = [] #주소 저장\n",
        "tw_id2 = [] #아이디 중복 확인을 위한 저장 공간\n",
        "last_id = -1 # ID of last tweet seen\n",
        "nouns_list2 = [] # 한글만 뽑은 문장에서 실제 단어인지 검사\n",
        "ko_text2 = [] # 파악한 단어 한글만 뽑기\n",
        "snstype2 = [] #snstype넣기\n",
        "modify_time2 = [] # 검색시간\n",
        "time_now = datetime.datetime.now() #검색시간 패키지 진값\n",
        "media_list2 = [] \n",
        "search_word2 = []\n",
        "id_str_list2 = [] #id값 저장칸\n",
        "\n",
        "\n",
        "#--------------------------------05/23 추가-----------------------------------\n",
        "main_keywords = df1['main_set'] #메인 키워드, CSV에서 Main 컬럼 호출\n",
        "sub_keywords = df1['sub_set'] # 서브 컬럼 호출\n",
        "main_keywords2=df_F['a']\n",
        "sub_keywords2=df_F['b']\n",
        "#--------------------------------05/23 추가-----------------------------------\n",
        "\n",
        "count_main = 0 #메인 단어 리스트 카운트 값\n",
        "count_sub = 0 # 서브 카운트 값\n",
        "result_main = 0 #메인 값을 집어넣음\n",
        "result_sub = 0 # 서브 단어를 집어넣음\n",
        "search_word = [] #검색단어 저장\n",
        "Done = 0 #main 종료를 위한 값\n",
        "Exit = 0 #완전 종료를 위한 값\n",
        "abc=0 #media 갯수 초기화 사용\n",
        "test = 0 #num대입을 위한 값\n",
        "\n",
        "print(df1['main_set'])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0                 대딸방\n",
            "1                 룸싸롱\n",
            "2               섹스파트너\n",
            "3                  콜걸\n",
            "4                 마사지\n",
            "5                  안마\n",
            "6                  출장\n",
            "7                  후기\n",
            "8                  셔츠\n",
            "9                 노래방\n",
            "10                파트너\n",
            "11                 추천\n",
            "12    Samanth72483370\n",
            "13    SusanPr33929337\n",
            "14    oartearz4yxWzDy\n",
            "15          MouyKelly\n",
            "16      babyfat_cheek\n",
            "17      Lesli86294362\n",
            "18    Dvy9YigoGtyi0SN\n",
            "19    W3EPNyF7FpUX3fR\n",
            "20    EsnnqZaLZqkgas2\n",
            "21    KrWMc5llH3ifTK6\n",
            "22    y7o2AqV9hUKJBOv\n",
            "Name: main_set, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "9w62vSafU-Og",
        "scrolled": true,
        "outputId": "759a7a47-d37f-445b-fd58-ab7773728788"
      },
      "source": [
        "#for i in main_keywords:  # 메인 키워드들의 단어를 하나씩 실행\n",
        "\n",
        "    #if not check_nan(main_keywords[count_main]):  # nan수 검사 만약 nan이 없다면 시작\n",
        "\n",
        "\n",
        "# 메인값 반복을 위한 while문 (메인 키워드의 갯수가 count_main 보다 높을 동안 계속 작동)\n",
        "while count_main < len(main_keywords):\n",
        "                        # 메인 단어를 result_main에저장\n",
        "            result_main = main_keywords[count_main]\n",
        "            count_sub = 0\n",
        "            \n",
        "            while count_sub < len(sub_keywords):\n",
        "                    \n",
        "                # 서브 단어를 result_sub에 저장\n",
        "                result_sub = sub_keywords[count_sub]\n",
        "                last_id = -1  # 다시 last id 초기화\n",
        "                test = num_needed\n",
        "                \n",
        "                #while len(tweet_list) < num_needed:  # 트위터 검색\n",
        "                while test > 0:  # 트위터 검색\n",
        "                    try:\n",
        "                        \n",
        "                        new_tweets = api.search(q=\"{}+' '+{}\".format(\n",
        "                            result_main, result_sub), count=num_needed, max_id=str(last_id - 1), tweet_mode='extended')\n",
        "                        \n",
        "                    except tweepy.TweepError as e:\n",
        "                        print(\"Error\", e)\n",
        "                        break\n",
        "                    else:\n",
        "                        if not new_tweets:\n",
        "                            print(\"Searching is done, Words is :\" + result_main +','+ result_sub)\n",
        "                            break\n",
        "                        else:\n",
        "                            for tweet in new_tweets:\n",
        "                                \n",
        "                                id_str = tweet.id_str\n",
        "                                id_str_list.append(id_str)\n",
        "                                \n",
        "                                screen_name = tweet.author.screen_name\n",
        "                                user_name = tweet.author.name\n",
        "                                tweet_text = tweet.full_text\n",
        "                                tweet_list.append(tweet_text)\n",
        "                                name_list.append(tweet.user.name)\n",
        "                                screen_name_list.append(screen_name)\n",
        "\n",
        "                                created_time = str(tweet.created_at)\n",
        "                                time_split = created_time.split()\n",
        "                                date_list.append(time_split[0])\n",
        "                                time_list.append(time_split[1])\n",
        "                                url_list.append('https://twitter.com/' + tweet.user.screen_name)\n",
        "                                tw_id.append(tweet.id)\n",
        "                                search_word.append(result_main + ' ' + result_sub)\n",
        "                                snstype.append('Twiiter')\n",
        "                                modify_time.append(time_now)\n",
        "                                \n",
        "                                nouns_list = (tweet.id) #공간 활용을 위한 허수값 넣기\n",
        "                                ko_text = (tweet.id) #공간 활용을 위한 허수값 넣기\n",
        "                             \n",
        "                                \n",
        "                                try:  \n",
        "                                    media_list.append(tweet.extended_entities['media'][abc]['media_url'])\n",
        "            \n",
        "                                except:\n",
        "                                    abc = \"no_media_url\"\n",
        "                                    media_list.append(abc)\n",
        "                         \n",
        "                                test = test - 1\n",
        "                                abc=0\n",
        "                               \n",
        "                    last_id = min(tw_id)  # 여기까지가 트위터 검색\n",
        "                count_sub = count_sub +1     \n",
        "                \n",
        "            count_main = count_main + 1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6df648ad6695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                         new_tweets = api.search(q=\"{}+' '+{}\".format(\n\u001b[0;32m---> 24\u001b[0;31m                             result_main, result_sub), count=num_needed, max_id=str(last_id - 1), tweet_mode='extended')\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTweepError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m                                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_on_rate_limit_notify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                                         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rate limit reached. Sleeping for: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msleep_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# sleep for few extra sec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0;31m# if self.wait_on_rate_limit and self._reset_time is not None and \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzwt7DyGuqBn"
      },
      "source": [
        "# ocr 검색"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9aNkOLKumr3"
      },
      "source": [
        "num_needed2 = 20\n",
        "tweet_list2 = [] # 트위터 내용\n",
        "name_list2 = [] #닉네임\n",
        "screen_name_list2 = [] #사용자 아이디\n",
        "date_list2 = [] #날짜 저장\n",
        "time_list2 = [] #트위터 시간 저장\n",
        "url_list2 = [] #주소 저장\n",
        "tw_id2 = [] #아이디 중복 확인을 위한 저장 공간\n",
        "last_id2 = -1 # ID of last tweet seen\n",
        "nouns_list2 = [] # 한글만 뽑은 문장에서 실제 단어인지 검사\n",
        "ko_text2 = [] # 파악한 단어 한글만 뽑기\n",
        "snstype2 = [] #snstype넣기\n",
        "modify_time2 = [] # 검색시간\n",
        "time_now2 = datetime.datetime.now() #검색시간 패키지 진값\n",
        "media_list2 = [] #사진 링크\n",
        "id_str_list2 = [] #id값 저장칸\n",
        "\n",
        "tweet_list2 = [] # 트위터 내용\n",
        "name_list2 = [] #닉네임\n",
        "screen_name_list2 = [] #사용자 아이디\n",
        "date_list2 = [] #날짜 저장\n",
        "time_list2 = [] #트위터 시간 저장\n",
        "url_list2 = [] #주소 저장\n",
        "tw_id2 = [] #아이디 중복 확인을 위한 저장 공간\n",
        "last_id2 = -1 # ID of last tweet seen\n",
        "nouns_list2 = [] # 한글만 뽑은 문장에서 실제 단어인지 검사\n",
        "ko_text2 = [] # 파악한 단어 한글만 뽑기\n",
        "snstype2 = [] #snstype넣기\n",
        "modify_time2 = [] # 검색시간\n",
        "time_now2 = datetime.datetime.now() #검색시간 패키지 진값\n",
        "media_list2 = [] \n",
        "search_word2 = []\n",
        "id_str_list2 = [] #id값 저장칸\n",
        "\n",
        "\n",
        "#--------------------------------05/23 추가-----------------------------------\n",
        "main_keywords2=df_F['a']\n",
        "sub_keywords2=df_F['b']\n",
        "#--------------------------------05/23 추가-----------------------------------\n",
        "\n",
        "count_main2 = 0 #메인 단어 리스트 카운트 값\n",
        "count_sub2 = 0 # 서브 카운트 값\n",
        "result_main2 = 0 #메인 값을 집어넣음\n",
        "result_sub2 = 0 # 서브 단어를 집어넣음\n",
        "search_word2 = [] #검색단어 저장\n",
        "Done2 = 0 #main 종료를 위한 값\n",
        "Exit2 = 0 #완전 종료를 위한 값\n",
        "abc2=0 #media 갯수 초기화 사용\n",
        "test2 = 0 #num대입을 위한 값\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykp36nLJJUgD"
      },
      "source": [
        "#for i in main_keywords:  # 메인 키워드들의 단어를 하나씩 실행\n",
        "\n",
        "    #if not check_nan(main_keywords[count_main]):  # nan수 검사 만약 nan이 없다면 시작\n",
        "\n",
        "\n",
        "# 메인값 반복을 위한 while문 (메인 키워드의 갯수가 count_main 보다 높을 동안 계속 작동)\n",
        "while count_main2 < len(main_keywords2):\n",
        "                        # 메인 단어를 result_main에저장\n",
        "            result_main2 = main_keywords2[count_main2]\n",
        "            count_sub2 = 0\n",
        "            \n",
        "            while count_sub2 < len(sub_keywords2):\n",
        "                    \n",
        "                # 서브 단어를 result_sub에 저장\n",
        "                result_sub2 = sub_keywords2[count_sub2]\n",
        "                last_id2 = -1  # 다시 last id 초기화\n",
        "                test2 = num_needed2\n",
        "                \n",
        "                #while len(tweet_list) < num_needed:  # 트위터 검색\n",
        "                while test2 > 0:  # 트위터 검색\n",
        "                    try:\n",
        "                        \n",
        "                        new_tweets2 = api.search(q=\"{}+' '+{}\".format(\n",
        "                            result_main2, result_sub2), count2=num_needed2, max_id2=str(last_id2 - 1), tweet_mode='extended')\n",
        "                        \n",
        "                    except tweepy.TweepError as e:\n",
        "                        print(\"Error\", e)\n",
        "                        break\n",
        "                    else:\n",
        "                        if not new_tweets2:\n",
        "                            print(\"Searching is done, Words is :\" + result_main2 +','+ result_sub2)\n",
        "                            break\n",
        "                        else:\n",
        "                            for tweet in new_tweets2:\n",
        "                                \n",
        "                                \n",
        "                                id_str2 = tweet.id_str2\n",
        "                                id_str_list2.append(id_str2)\n",
        "                                \n",
        "                                screen_name2 = tweet.author.screen_name\n",
        "                                user_name2 = tweet.author.name\n",
        "                                tweet_text2 = tweet.full_text\n",
        "                                tweet_list2.append(tweet_text2)\n",
        "                                name_list2.append(tweet.user.name)\n",
        "                                screen_name_list2.append(screen_name2)\n",
        "                                \n",
        "\n",
        "                              \n",
        "                                created_time2 = str(tweet.created_at)\n",
        "                                time_split2 = created_time.split()\n",
        "                                date_list2.append(time_split[0])\n",
        "                                time_list2.append(time_split[1])\n",
        "                                url_list2.append('https://twitter.com/' + tweet.user.screen_name)\n",
        "                                tw_id2.append(tweet.id)\n",
        "                                search_word2.append(result_main + ' ' + result_sub)\n",
        "                                snstype2.append('Twiiter')\n",
        "                                modify_time2.append(time_now)\n",
        "                                \n",
        "                                nouns_list2 = (tweet.id) #공간 활용을 위한 허수값 넣기\n",
        "                                ko_text2 = (tweet.id) #공간 활용을 위한 허수값 넣기\n",
        "                                \n",
        "                                try:  \n",
        "                                    media_list2.append(tweet.extended_entities['media'][abc2]['media_url'])\n",
        "            \n",
        "                                except:\n",
        "                                    abc2 = \"no_media_url\"\n",
        "                                    media_list2.append(abc)\n",
        "                         \n",
        "                                test2 = test2 - 1\n",
        "                                abc2=0\n",
        "                               \n",
        "                                test2 = test2 - 1                          \n",
        "                               \n",
        "                    last_id2 = min(tw_id2)  # 여기까지가 트위터 검색\n",
        "                count_sub2 = count_sub2 +1     \n",
        "                \n",
        "            count_main2 = count_main2 + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixz45hwpU-Oh",
        "scrolled": true
      },
      "source": [
        "# RT 제거,데이터 중복제거,데이터 프레임 저장하기\n",
        "\n",
        "df2 = pd.DataFrame({'아이디':screen_name_list,\n",
        "                   '닉네임':name_list,\n",
        "                   '내용':tweet_list,\n",
        "                   '날짜':date_list,\n",
        "                   '시간':time_list,\n",
        "                   '주소':url_list,\n",
        "                   '검색단어':search_word,\n",
        "                   'nouns':nouns_list,\n",
        "                   'ko_text':ko_text,\n",
        "                   'SNS타입':snstype,\n",
        "                   '현재 시간': modify_time,\n",
        "                   '사진': media_list,\n",
        "                   'id_str':id_str_list\n",
        "                  })\n",
        "df2 = df2[~df2.내용.str.contains(\"RT @\")]\n",
        "df2 = df2.drop_duplicates(['아이디'])\n",
        "print(df2)\n",
        "\n",
        "\n",
        "# df2는 서치한 트윗의 아이디, 닉네임, 내용, 날짜, 시간, 주소, 검색단어, nouns, ko_text 열이 있는 db\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwkrzHZHJUgF"
      },
      "source": [
        "\n",
        "\n",
        "df2_2 = pd.DataFrame({'아이디':screen_name_list2,\n",
        "                   '닉네임':name_list2,\n",
        "                   '내용':tweet_list2,\n",
        "                   '날짜':date_list2,\n",
        "                   '시간':time_list2,\n",
        "                   '주소':url_list2,\n",
        "                   '검색단어':search_word2,\n",
        "                   'nouns':nouns_list2,\n",
        "                   'ko_text':ko_text2,\n",
        "                   'SNS타입':snstype2,\n",
        "                   '현재 시간': modify_time2,\n",
        "                   '사진': media_list2,\n",
        "                   'id_str':id_str_list2\n",
        "                  })\n",
        "print(df2_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-LhdPh-JUgF"
      },
      "source": [
        "#RT 제거\n",
        "if not df2_2.empty:\n",
        "  df2_2 = df2_2[~df2_2.내용.str.contains(\"RT @\")]\n",
        "else:\n",
        "  df2_2 = df2_2\n",
        "\n",
        "df2_2 = df2_2.drop_duplicates(['아이디'])\n",
        "print(df2_2)\n",
        "\n",
        "# df2는 서치한 트윗의 아이디, 닉네임, 내용, 날짜, 시간, 주소, 검색단어, nouns, ko_text 열이 있는 db\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHVgjax7JUgF"
      },
      "source": [
        "df3 = pd.concat([df2,df2_2])\n",
        "print(df3)\n",
        "\n",
        "#df3는 기존 키워드랑 변형 키워드로 서치해서 탐지한 모든 트윗들"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkLd5gEgU-Oh"
      },
      "source": [
        "#디비 twitter 데이터 입력\n",
        "for i in range(0, len(df3)):\n",
        "    sql = \"INSERT INTO project.twitter(id,nickname,contents,date,time,url,search_word,ko_text,snstype,modify_time,id_str,media_url) VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s);\"\n",
        "    values = df3['아이디'].iloc[i], df3['닉네임'].iloc[i], df3['내용'].iloc[i], df3['날짜'].iloc[i], df3['시간'].iloc[i], df3['주소'].iloc[i], df3['검색단어'].iloc[i], df3['ko_text'].iloc[i],df3['SNS타입'].iloc[i],time_now,df3['id_str'].iloc[i],df3['사진'].iloc[i]\n",
        "    cursor.execute(sql,values)\n",
        "    data_words_db.commit()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTrruY6sU-Oi"
      },
      "source": [
        "#print(tweet_list)\n",
        "def text_cleaning(text):\n",
        "  hangul = re.compile('[^ ㄱ-ㅣ가-힣]+') # 한글만 뽑는 정규표현식 (수정함)\n",
        "  rtnono = re.compile('[RT]+')\n",
        "  result = hangul.sub('',text)\n",
        "  result = rtnono.sub('',text) # 공백으로 처리\n",
        "  return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNt3UQvTU-Oi",
        "scrolled": true
      },
      "source": [
        "# df2는 서치한 트윗의 아이디, 닉네임, 내용, 날짜, 시간, 주소, 검색단어, nouns, ko_text 열이 있는 db\n",
        "\n",
        "df2['ko_text'] = df2['내용'].apply(lambda x: text_cleaning(x))\n",
        "\n",
        "#--------------------------------05/23 추가-----------------------------------\n",
        "#korean_stopwords_path = addr + 'korean_stopwords.txt' \n",
        "\n",
        "sql = \"SELECT * FROM data_stopword;\"\n",
        "cursor.execute(sql)\n",
        "result = cursor.fetchall()\n",
        "\n",
        "data_stopword = pd.DataFrame(result) # 데이터프레임 생성\n",
        "data_stopword.columns=['number','stopword'] # 숫자 열과 stopword 열이 있음.\n",
        "print(data_stopword)\n",
        "\n",
        "stopwords = data_stopword['stopword']\n",
        "#with open(korean_stopwords_path, encoding='UTF8') as f:\n",
        "#  stopwords = f.readlines() # 불용어 파일 읽기\n",
        "\n",
        "\n",
        "#--------------------------------05/23 추가-----------------------------------\n",
        "stopwords = [x.strip() for x in stopwords] \n",
        "\n",
        "# (2/2) nouns 생성:의미있는 명사만 추출하는 함수 생성\n",
        "def get_nouns(x):\n",
        "  nouns_tagger = Okt()\n",
        "  nouns = nouns_tagger.nouns(x)\n",
        "  # 한 글자 키워드 제거\n",
        "  nouns = [noun for noun in nouns if len(noun) > 1]\n",
        "  # 불용어 제거\n",
        "  nouns = [noun for noun in nouns if noun not in stopwords]\n",
        "  return nouns\n",
        "\n",
        "#ko_text에 이를 적용\n",
        "#df['nouns'] = df['ko_text'].apply(lambda x: get_nouns(x)) # 생성한 ko_text파일 다시 전처리\n",
        "print(df3.주소)\n",
        "df3['nouns'] = df2['ko_text'].apply(lambda x: get_nouns(x))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFnBvhoPU-Oi"
      },
      "source": [
        "# DataFrame 에 들어있는 자료들을 확인하기 위해 상위 n개 자료를 보여줌\n",
        "\n",
        "# 키워드 빈도 추출하기\n",
        "# 말뭉치 추출\n",
        "tweet_corpus = \"\".join(df2['ko_text'].tolist())\n",
        "# 명사 키워드 추출\n",
        "nouns_tagger = Okt()\n",
        "nouns = nouns_tagger.nouns(tweet_corpus)\n",
        "count = Counter(nouns)\n",
        "# 한 글자 키워드 제거\n",
        "remove_char_counter = Counter({x:count[x] for x in count if len(x) > 1}) # Counter는 같은 단어가 몇개 들어있는지 알려줌"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-KlJc9PU-Oj",
        "scrolled": true
      },
      "source": [
        "# 키워드와 키워드 빈도 점수를 'df_howMany', 'df_size'라는 데이터 프레임의 피처로 생성\n",
        "df_howMany = pd.DataFrame(remove_char_counter.items(),columns = ['node_old','nodesize_old'])\n",
        "\n",
        "df_howMany_100up = df_howMany[df_howMany['nodesize_old'] >= 50]\n",
        "#빈도수 50 이상만 출력\n",
        "df_howMany_100up_sort = df_howMany_100up.sort_values(by='nodesize_old' ,ascending=False)\n",
        "print(df_howMany_100up_sort)\n",
        "#위에 명령어가 빈도수 제거인데 홀수개가 나올 수 있으므로 일단 제거\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftVHq04pBwkW"
      },
      "source": [
        "\n",
        "# 리스트에 빈도수 100개 이상인 키워드 넣기.\n",
        "'''top_lists = []\n",
        "for nod in df_howMany_100up_sort:\n",
        "  top_lists.append(nod)\n",
        "print('\\n')\n",
        "print(top_lists)'''\n",
        "# 여기서 도시 명을 제거할거임. 도시명은 불법광고 탐지에 도움이 안되므로.\n",
        "# 예시: top_lists=['오피', '출장', '만남']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSQUrCHsMHh6",
        "scrolled": true
      },
      "source": [
        "# 예시: lists=['오피', '출장', '만남']\n",
        "\n",
        "# print(result)\n",
        "#--------------------------------05/23 추가-----------------------------------\n",
        "\n",
        "sql = \"SELECT * FROM data_country;\"\n",
        "cursor.execute(sql)\n",
        "result = cursor.fetchall()\n",
        "\n",
        "\n",
        "data_country = pd.DataFrame(result) # 데이터프레임 만들기. 숫자 열 + 도시 이름 열\n",
        "data_country.columns=['number','country_name']\n",
        "#print(data_country['country_name'])\n",
        "\n",
        "#korean_city_names_path = (addr + '전국도시이름.txt')\n",
        "#with open(korean_city_names_path,encoding='utf8') as f:\n",
        "cities = data_country['country_name']\n",
        "cities = [x.strip() for x in cities]\n",
        "# cities = ['서울', '부산', '인천', '대구' 등 100~200개 써놓음]\n",
        "#print(cities)\n",
        "\n",
        "# 도시 이름 제거\n",
        "for i in cities:\n",
        "    not_cities = df_howMany_100up_sort[df_howMany_100up_sort.node_old != i]\n",
        "\n",
        "#--------------------------------05/23 추가-----------------------------------\n",
        "df4 = pd.DataFrame(not_cities)\n",
        "df4.columns = ['node_old','nodesize_old']\n",
        "\n",
        "print(df3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jsjJwukJUgH"
      },
      "source": [
        "#수집된 단어 정보를 워드클라우드 추가 하기 위한 부분\n",
        "\n",
        "sql = \"SELECT * FROM words_cloud;\"\n",
        "cursor.execute(sql)\n",
        "result = cursor.fetchall()\n",
        "wd = pd.DataFrame(result)\n",
        "wd.columns = ['node_old','nodesize_old']\n",
        "\n",
        "print(wd)\n",
        "\n",
        "new_words = df4['node_old']\n",
        "new_words_size = df4['nodesize_old']\n",
        "\n",
        "\n",
        "words_cloud = pd.concat([wd,df4], ignore_index=True)\n",
        "words_cloud = words_cloud.groupby(by=['node_old'],as_index=False).sum()\n",
        "words_cloud = words_cloud.sort_values(by=['nodesize_old'],axis=0, ascending=False)\n",
        "print(words_cloud)\n",
        "\n",
        "for i in range(0, len(words_cloud)):\n",
        "    \n",
        "    sql = \"INSERT INTO `words_cloud`(`words`, `words_size`) VALUES(%s, %s) ON DUPLICATE KEY UPDATE `words_size`=VALUES(`words_size`);\"\n",
        "    values = words_cloud['node_old'].iloc[i],words_cloud['nodesize_old'].iloc[i]\n",
        "    cursor.execute(sql,values)\n",
        "    data_words_db.commit()                                                                                                          \n",
        "                                                                                                                \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7ranA6DU-Oj"
      },
      "source": [
        "#단어 합치고 비교하는 곳\n",
        "#print(df_final) # 기존 단어셋 한줄로 합친 거\n",
        "#print('\\n')\n",
        "#print(df3) # 최종적으로 뽑은 단어셋\n",
        "lst = []\n",
        "for i in range(len(df4)):\n",
        "  lst.append(df3.iloc[i][0])\n",
        "#최종으로 뽑은 단어 1st 공간에 넣기\n",
        "\n",
        "df5 = pd.DataFrame(lst)\n",
        "#리스트 데이터 프레임으로 변형\n",
        "print(df5)\n",
        "print('\\n')\n",
        "\n",
        "#df_final = 기존 단어셋(main과 sub) 합친 것\n",
        "real_final1 = df_final.drop_duplicates()\n",
        "#real_final1 = 기존 단어셋에서 중복 제거를 한 것\n",
        "real_final1=real_final1.reset_index(drop=True) # 인덱스 초기화해줌\n",
        "\n",
        "\n",
        "#1sts에 중복제거된 기존 단어셋 저장\n",
        "lsts=[]\n",
        "for i in df_final['node_name']:\n",
        "  lsts.append(i)\n",
        "\n",
        "#old = 예전 단어셋\n",
        "#new = 새로 수집된 단어셋\n",
        "#final = 새로운 단어셋에서 오래된 것을 빼서 새로운 단어만 파악\n",
        "old=set(lsts)\n",
        "new=set(lst)\n",
        "final=new-old\n",
        "\n",
        "\n",
        "new_words_list=list(final)\n",
        "#final을 리스트 형태에 삽입\n",
        "\n",
        "\n",
        "df6=pd.DataFrame(new_words_list)\n",
        "df6.columns=['node_name']\n",
        "#new_words_list를 dateframe형태로 변환 후 컬럼 네임 부여\n",
        "print(df6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvmrIh7OU-Ok"
      },
      "source": [
        "\n",
        "#빈도수 확인된 단어를 짝수로 맞추고 쪼개는 부분\n",
        "sql = \"SELECT * FROM data_set;\"\n",
        "cursor.execute(sql)\n",
        "result = cursor.fetchall()\n",
        "df = pd.DataFrame(result)\n",
        "df.columns = ['number','main_set','sub_set']\n",
        "\n",
        "l = len(df6)\n",
        "#print(l)\n",
        "if  l % 2 != 0:\n",
        "  df7 = df6.drop(df6.index[-1])\n",
        "else:\n",
        "  df7 = df6\n",
        "\n",
        "\n",
        "test1 = df7['node_name'].sample(frac=0.5, random_state=2019)\n",
        "test2 = df7['node_name'].drop(test1.index)\n",
        "\n",
        "\n",
        "for i in range(0, test2.shape[0]):\n",
        "    l=test2.iloc[i]\n",
        "    o=test1.iloc[i]\n",
        "    sql = \"INSERT INTO project.data_set(main_set,sub_set) VALUES(%s,%s);\"\n",
        "    values = l,o\n",
        "    cursor.execute(sql,values)\n",
        "    data_words_db.commit()\n",
        "    \n",
        "data_words_db.close() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhVflZr9U-Ok"
      },
      "source": [
        "# csv 파일 생성하기\n",
        "\n",
        "#final_csv = pd.DataFrame(node_df)\n",
        "#final_csv.to_excel(addr + '최종키워드'+'.xlsx')\n",
        "#pd.read_excel(addr + '최종키워드' + '.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZp1Cef2U-Ok"
      },
      "source": [
        "# # 파일로 저장\n",
        "# df.to_csv(addr + '최종파일'+'.csv', encoding='utf-8-sig')\n",
        "# print(df)\n",
        "\n",
        "\n",
        "# #--------------------------------05/23 추가-----------------------------------\n",
        "# data_words_db.close()\n",
        "# #--------------------------------05/23 추가-----------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6syrldoqNd0y"
      },
      "source": [
        "# OCR 부분"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Iwh2J7iNjJm"
      },
      "source": [
        "#DB 연동\n",
        "data_words_db = pymysql.connect(\n",
        "    user='py',\n",
        "    passwd='1q2w3e4r!@#',\n",
        "    host='database-1.csxshuumxmig.ap-northeast-2.rds.amazonaws.com',\n",
        "    db='project',\n",
        "    charset='utf8mb4',\n",
        "    use_unicode=True\n",
        ")\n",
        "\n",
        "#DB 접속\n",
        "cursor = data_words_db.cursor()\n",
        "sql = \"SELECT * FROM data_normal_test;\"\n",
        "cursor.execute(sql)\n",
        "result = cursor.fetchall()\n",
        "df_n = pd.DataFrame(result)\n",
        "df_n = pd.DataFrame(result,columns = ['number','main_set','sub_set'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXWDFuRxNnkM"
      },
      "source": [
        "df_n_1=pd.DataFrame()\n",
        "# 빈 데이터 프레임 생성\n",
        "df_n_sum = pd.concat([df_n['main_set'],df_n['sub_set']])\n",
        "#df1_sum에 기존의 단어셋인 main과 sub를 결합\n",
        "df_n_sum=df_n_sum.reset_index(drop=True) \n",
        "#인덱스 초기화를 통해 컬럼의 row 번호를 재정렬하고 drop=true로 컬럼명 제거\n",
        "#print(df_n_sum)\n",
        "#print('\\n')\n",
        "\n",
        "df_final=pd.concat([df_n_1,df_n_sum],ignore_index=True)\n",
        "#df_final에 빈프레임(df_n_1)과 기존 단어셋을 합친다(df_n_sum)\n",
        "df_final.columns=['node_name']\n",
        "#df_final의 컬럼명을 node_name으로 정립\n",
        "#print(df_final)\n",
        "\n",
        "# df_n은 main_set과 sub_set이 있는 키워드 db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1FfWFjgNpWV"
      },
      "source": [
        "def check_nan(value) :\n",
        "    try :\n",
        "        float(value)\n",
        "        return True\n",
        "    except ValueError :\n",
        "        return False\n",
        "\n",
        "\n",
        "num_needed = 20\n",
        "tweet_list = [] # 트위터 내용\n",
        "name_list = [] #닉네임\n",
        "screen_name_list = [] #사용자 아이디\n",
        "date_list = [] #날짜 저장\n",
        "time_list = [] #트위터 시간 저장\n",
        "url_list = [] #주소 저장\n",
        "tw_id = [] #아이디 중복 확인을 위한 저장 공간\n",
        "last_id = -1 # ID of last tweet seen\n",
        "nouns_list = [] # 한글만 뽑은 문장에서 실제 단어인지 검사\n",
        "ko_text = [] # 파악한 단어 한글만 뽑기\n",
        "snstype = [] #snstype넣기\n",
        "modify_time = [] # 검색시간\n",
        "time_now = datetime.datetime.now() #검색시간 패키지 진값\n",
        "media_list = [] #사진 링크\n",
        "id_str_list = [] #id값 저장칸\n",
        "\n",
        "#-------------------------------------------------------------------\n",
        "main_keywords = df_n['main_set'] \n",
        "sub_keywords = df_n['sub_set'] \n",
        "#-------------------------------------------------------------------\n",
        "\n",
        "count_main = 0 #메인 단어 리스트 카운트 값\n",
        "count_sub = 0 # 서브 카운트 값\n",
        "result_main = 0 #메인 값을 집어넣음\n",
        "result_sub = 0 # 서브 단어를 집어넣음\n",
        "search_word = [] #검색단어 저장\n",
        "Done = 0 #main 종료를 위한 값\n",
        "Exit = 0 #완전 종료를 위한 값\n",
        "\n",
        "test = 0\n",
        "\n",
        "#print(df_n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9et1WSDoNrwW"
      },
      "source": [
        "# 메인값 반복을 위한 while문 (메인 키워드의 갯수가 count_main 보다 높을 동안 계속 작동)\n",
        "while count_main < len(main_keywords):\n",
        "                        # 메인 단어를 result_main에저장\n",
        "            result_main = main_keywords[count_main]\n",
        "            count_sub = 0\n",
        "            \n",
        "            while count_sub < len(sub_keywords):\n",
        "                    \n",
        "                # 서브 단어를 result_sub에 저장\n",
        "                result_sub = sub_keywords[count_sub]\n",
        "                last_id = -1  # 다시 last id 초기화\n",
        "                test = num_needed\n",
        "                \n",
        "                #while len(tweet_list) < num_needed:  # 트위터 검색\n",
        "                while test > 0:  # 트위터 검색\n",
        "                    try:\n",
        "                        \n",
        "                        new_tweets = api.search(q=\"{}+' '+{}\".format(\n",
        "                            result_main, result_sub), count=num_needed, max_id=str(last_id - 1), tweet_mode='extended')\n",
        "                        \n",
        "                    except tweepy.TweepError as e:\n",
        "                        print(\"Error\", e)\n",
        "                        break\n",
        "                    else:\n",
        "                        if not new_tweets:\n",
        "                            print(\"Searching is done, Words is :\" + result_main +','+ result_sub)\n",
        "                            break\n",
        "                        else:\n",
        "                            for tweet in new_tweets:\n",
        "                                \n",
        "                                id_str = tweet.id_str\n",
        "                                id_str_list.append(id_str)\n",
        "                                \n",
        "                                screen_name = tweet.author.screen_name\n",
        "                                user_name = tweet.author.name\n",
        "                                tweet_text = tweet.full_text\n",
        "                                tweet_list.append(tweet_text)\n",
        "                                name_list.append(tweet.user.name)\n",
        "                                screen_name_list.append(screen_name)\n",
        "\n",
        "                                created_time = str(tweet.created_at)\n",
        "                                time_split = created_time.split()\n",
        "                                date_list.append(time_split[0])\n",
        "                                time_list.append(time_split[1])\n",
        "                                url_list.append('https://twitter.com/' + tweet.user.screen_name)\n",
        "                                tw_id.append(tweet.id)\n",
        "                                search_word.append(result_main + ' ' + result_sub)\n",
        "                                snstype.append('Twiiter')\n",
        "                                modify_time.append(time_now)\n",
        "                                \n",
        "                                nouns_list = (tweet.id) #공간 활용을 위한 허수값 넣기\n",
        "                                ko_text = (tweet.id) #공간 활용을 위한 허수값 넣기\n",
        "                             \n",
        "                                \n",
        "                                try:  \n",
        "                                    media_list.append(tweet.extended_entities['media'][abc]['media_url'])\n",
        "            \n",
        "                                except:\n",
        "                                    abc = \"no_media_url\"\n",
        "                                    media_list.append(abc)\n",
        "                         \n",
        "                                test = test - 1\n",
        "                                abc=0\n",
        "                               \n",
        "                    last_id = min(tw_id)  # 여기까지가 트위터 검색\n",
        "                count_sub = count_sub +1     \n",
        "                \n",
        "            count_main = count_main + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLa4YRnGNuBV"
      },
      "source": [
        "\n",
        "df_n_1 = pd.DataFrame({'아이디':screen_name_list,\n",
        "                   '닉네임':name_list,\n",
        "                   '날짜':date_list,\n",
        "                   '시간':time_list,\n",
        "                   '주소':url_list,\n",
        "                   '검색단어':search_word,\n",
        "                   'SNS타입':snstype,\n",
        "                   '현재 시간': modify_time,\n",
        "                   '사진': media_list\n",
        "                  })\n",
        "df_n_1 = df_n_1.drop_duplicates(['아이디'])\n",
        "#print(df_n_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxgZHxWBNx0u"
      },
      "source": [
        "## API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp4PHEFANv5M"
      },
      "source": [
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'axiomatic-atlas-316112-69425609322a.json'\n",
        "import urllib.request\n",
        "import io\n",
        "from google.cloud import vision\n",
        "import ssl\n",
        "\n",
        "#계정 정보 얻어옴\n",
        "client = vision.ImageAnnotatorClient()\n",
        "\n",
        "#사진 읽어오기 \n",
        "lsts=[]\n",
        "#def ocr(url):\n",
        "client = vision.ImageAnnotatorClient()\n",
        "for i in df_n_1['사진']:\n",
        "  if i != 'no_media_url':\n",
        "    with urllib.request.urlopen(i) as image_file: \n",
        "      content = image_file.read()\n",
        "      lsts.append(content)\n",
        "  else: lsts.append('None')\n",
        "#print(len(lsts)) \n",
        "#print(len(df_n_1)) # 동일함. 잘 받아옴"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwE5q1MlNz3b"
      },
      "source": [
        "f_list=[]\n",
        "for j in lsts:\n",
        "  if j != 'None':\n",
        "    image = vision.Image(content=j)\n",
        "    response = client.text_detection(image=image) # 사진에서 텍스트 추출\n",
        "    texts = response.text_annotations\n",
        "    result = []\n",
        "    for text in texts:\n",
        "      result.append(text.description)\n",
        "    f_list.append(result)\n",
        "  else: f_list.append('none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH91N9zFN1pu"
      },
      "source": [
        "eList=[]\n",
        "for k in range(len(f_list)):\n",
        "  eList.append(f_list[k][0])\n",
        "#print(eList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VexsVu6TN2tL"
      },
      "source": [
        "url = df_n_1['주소']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duSUDZu3N4Xo"
      },
      "source": [
        "# columns=기존 트위터 DB 컬럼에 ocr text, ocr한 이미지 url 추가\n",
        "import pandas as pd\n",
        "\n",
        "url = list(url)\n",
        "print(url)\n",
        "print(len(url))\n",
        "\n",
        "oxList=[]\n",
        "for i in range(len(url)):\n",
        "  oxList.append('X')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d45gpoo4N6PO"
      },
      "source": [
        "df_ocr = pd.DataFrame({\n",
        "                        '주소':url,\n",
        "                        'ocr_text':eList,\n",
        "                      })\n",
        "#print(df_ocr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWDxzQqTN8JH"
      },
      "source": [
        "l_texts = list(df_ocr['ocr_text'])\n",
        "#print(l_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Zg9ClZN_po"
      },
      "source": [
        "## 기존 단어셋 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3s8l175N9mO"
      },
      "source": [
        "#DB 접속\n",
        "cursor = data_words_db.cursor()\n",
        "sql = \"SELECT * FROM data_set;\"\n",
        "cursor.execute(sql)\n",
        "result = cursor.fetchall()\n",
        "df1 = pd.DataFrame(result)\n",
        "df1.columns = ['number','main_set','sub_set']\n",
        "#df1 = 기존의 단어셋\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "df1_1=pd.DataFrame()\n",
        "# 빈 데이터 프레임 생성\n",
        "df1_sum = pd.concat([df1['main_set'],df1['sub_set']])\n",
        "#df1_sum에 기존의 단어셋인 main과 sub를 결합\n",
        "df1_sum=df1_sum.reset_index(drop=True) \n",
        "#인덱스 초기화를 통해 컬럼의 row 번호를 재정렬하고 drop=true로 컬럼명 제거\n",
        "\n",
        "df_final=pd.concat([df1_1,df1_sum],ignore_index=True)\n",
        "#df_final에 빈프레임(df1_1)과 기존 단어셋을 합친다(df1_sum)\n",
        "df_final.columns=['node_name']\n",
        "#df_final의 컬럼명을 node_name으로 정립\n",
        "#print(df_final)\n",
        "\n",
        "# df1은 main_set과 sub_set이 있는 키워드 db\n",
        "l_keys=[]\n",
        "for i in df_final['node_name']:\n",
        "  l_keys.append(i)\n",
        "#print(l_keys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3muFg5AOBoT"
      },
      "source": [
        "#print(l_texts)\n",
        "#print(l_keys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkD7ivhNOD3n"
      },
      "source": [
        "real_final=[]\n",
        "for i in l_texts:\n",
        "  for j in l_keys:\n",
        "    if j in i:\n",
        "      real_final.append(i)\n",
        "\n",
        "final_list=list(set(real_final))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKRte9gwOFA7"
      },
      "source": [
        "df_filter = pd.DataFrame(columns=['ocr_text'])\n",
        "df_filter['ocr_text'] = final_list\n",
        "#print(df_filter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3_QT3nbOHMd"
      },
      "source": [
        "df_sum = pd.concat([df_ocr,df_filter])\n",
        "df_sum=df_sum.reset_index(drop=True) # 인덱스 초기화해줌\n",
        "\n",
        "\n",
        "#print(df_filter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ6nsG9oOIMq"
      },
      "source": [
        "df_sum2 = pd.DataFrame(columns=['주소','ocr_text'])\n",
        "for i in df_filter['ocr_text']:\n",
        "  df_sum2 = df_sum.loc[df_sum['ocr_text']==i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyAJzPGtOImX"
      },
      "source": [
        "df_sum2 = pd.DataFrame(columns=['주소','ocr_text'])\n",
        "df_sum3 = pd.DataFrame(columns=['주소','ocr_text'])\n",
        "df_sum4 = pd.DataFrame(columns=['주소','ocr_text'])\n",
        "for i in df_filter['ocr_text']:\n",
        "  df_sum2 = df_sum.loc[df_sum['ocr_text']==i]\n",
        "  df_sum3 = df_sum3.append(df_sum2)\n",
        "  df_sum3=df_sum3.reset_index(drop=True) # 인덱스 초기화해줌\n",
        "  df_sum4 = df_sum3.iloc[::2,:]\n",
        "  df_sum4=df_sum4.reset_index(drop=True)\n",
        "#print(df_sum4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umvVFMEWOJyA"
      },
      "source": [
        "#print(df_n_1)\n",
        "#print('------------')\n",
        "#print(df_sum4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHDdKq5pOLSB"
      },
      "source": [
        "ocr_final = pd.merge(df_n_1, df_sum4, how='outer', on='주소')\n",
        "ocr_final = ocr_final.where((pd.notnull(ocr_final)), None) # nan값 none으로 바꾸기\n",
        "#ocr_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEvXhlo3ON-N"
      },
      "source": [
        "## DB 연동"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meNbBghrONsq"
      },
      "source": [
        "#디비 data_ocr 데이터 입력\n",
        "for i in range(0, len(df_n_1)):\n",
        "    sql = \"INSERT INTO project.data_ocr(id,nickname,date,time,url,search_word,snstype,modify_time,media_url,ocr_text) VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s);\"\n",
        "    values = ocr_final['아이디'].iloc[i], ocr_final['닉네임'].iloc[i], ocr_final['날짜'].iloc[i], ocr_final['시간'].iloc[i], ocr_final['주소'].iloc[i], ocr_final['검색단어'].iloc[i],ocr_final['SNS타입'].iloc[i],time_now,ocr_final['사진'].iloc[i], ocr_final['ocr_text'].iloc[i]\n",
        "    cursor.execute(sql,values)\n",
        "    data_words_db.commit()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
